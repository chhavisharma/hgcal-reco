{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from particle_margin import TrackMLParticleTrackingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events found =  100\n",
      "number of events read =  100\n",
      "CPU times: user 4 ms, sys: 8 ms, total: 12 ms\n",
      "Wall time: 7.37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = TrackMLParticleTrackingDataset(root='/home/csharma/prototyping/data/train_1/',\n",
    "                                      layer_pairs_plus=True,\n",
    "                                      pt_min=0,\n",
    "                                      n_events=100, n_workers=1)\n",
    "print('number of events read = ',len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_cluster import knn_graph, radius_graph\n",
    "from torch_scatter import scatter_mean, scatter_add\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import max_pool_x\n",
    "from torch_geometric.utils.undirected import to_undirected\n",
    "from torch_geometric.nn import EdgeConv\n",
    "from torch_geometric.typing import OptTensor, PairTensor\n",
    "from typing import List\n",
    "\n",
    "#https://github.com/eldridgejm/unionfind\n",
    "from unionfind import UnionFind\n",
    "\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "def simple_embedding_truth(coords, truth_label_by_hits, device='cpu'):\n",
    "    truth_ordering = torch.argsort(truth_label_by_hits)    \n",
    "    uniques, counts = torch.unique(truth_label_by_hits, return_counts=True)\n",
    "\n",
    "    out_truths: List[PairTensor] = []\n",
    "#     for cat in uniques[:-1]:\n",
    "    for cat in uniques:\n",
    "        thecat = cat.item()\n",
    "        in_cat = coords[truth_label_by_hits == thecat]\n",
    "        not_cat = coords[truth_label_by_hits != thecat]\n",
    "        \n",
    "        in_cat_dists = torch.cdist(in_cat, in_cat)\n",
    "        in_idxs = torch.triu_indices(in_cat_dists.size()[0], in_cat_dists.size()[0], \n",
    "                                     offset=1, device=in_cat.device)\n",
    "        in_idxs = in_idxs[0] + in_cat_dists.size()[0]*in_idxs[1]\n",
    "        in_cat_dists = in_cat_dists.view(-1)[in_idxs] / (uniques.size()[0] - 1)\n",
    "        \n",
    "        # all pairwise distances between in-category and out of category\n",
    "        # there's a factor of 2 here I need to deal with\n",
    "        not_cat_dists = torch.cdist(in_cat, not_cat).flatten() / (uniques.size()[0] - 1)\n",
    "                \n",
    "        #build the final labelled distance vectors\n",
    "        dists = torch.cat([in_cat_dists, not_cat_dists], dim=0)\n",
    "        truth = torch.cat([torch.ones_like(in_cat_dists, dtype=torch.int64),\n",
    "                           torch.full_like(not_cat_dists, -1, dtype=torch.int64)], dim=0)\n",
    "        out_truths.append((dists, truth))\n",
    "        \n",
    "    return out_truths\n",
    "\n",
    "import networkx as nx\n",
    "def match_cluster_targets(clusters, truth_clusters, data):\n",
    "    np_truth_clusters = truth_clusters.cpu().numpy()\n",
    "    true_cluster_labels = np.unique(np_truth_clusters)   \n",
    "    np_clusters = clusters.cpu().numpy()\n",
    "    pred_cluster_labels = np.unique(np_clusters)\n",
    "    pred_cluster_mask = np.ones_like(np_truth_clusters, dtype=np.bool)\n",
    "        \n",
    "    #print(data)    \n",
    "    \n",
    "    #print('match_cluster_targets')\n",
    "    #print(np_clusters)\n",
    "    #print(np_truth_clusters)\n",
    "    #print(true_cluster_labels)\n",
    "    #print(pred_cluster_labels)\n",
    "    indices = np.arange(np_clusters.size, dtype=np.int64)\n",
    "    #print(indices)\n",
    "    pred_clusters = []\n",
    "    for label in pred_cluster_labels:\n",
    "        pred_clusters.append(indices[np_clusters == label])\n",
    "    #print(pred_clusters)\n",
    "    \n",
    "    # make pt weighting vector\n",
    "    max_pt = torch.max(data.truth_pt).item()\n",
    "    #print(max_pt)\n",
    "    \n",
    "    matched_pred_clusters = []\n",
    "    true_cluster_properties = []\n",
    "    for label in true_cluster_labels:\n",
    "        true_indices = indices[np_truth_clusters == label]        \n",
    "        best_pred_cluster = -1\n",
    "        best_iou = 0\n",
    "        for i, pc in enumerate(pred_clusters):\n",
    "            isec = np.intersect1d(true_indices, pc)\n",
    "            iun = np.union1d(true_indices, pc)\n",
    "            iou = isec.size/iun.size\n",
    "            if best_pred_cluster == -1 or iou > best_iou:\n",
    "                best_pred_cluster = i\n",
    "                best_iou = iou\n",
    "        matched_pred_clusters.append(best_pred_cluster)\n",
    "        \n",
    "        # now make the properties vector\n",
    "        thebc = torch.unique(data.y_particle_barcodes[data.y == label]).item()\n",
    "        select_truth = (data.truth_barcodes == thebc)\n",
    "        true_cluster_properties.append(1./data.truth_pt[select_truth])\n",
    "        #[data.truth_eta[select_truth], data.truth_phi[select_truth]]\n",
    "    matched_pred_clusters = np.array(matched_pred_clusters, dtype=np.int64)\n",
    "    pred_indices = torch.from_numpy(matched_pred_clusters).to(clusters.device)\n",
    "    #print(pred_indices)\n",
    "    \n",
    "    true_cluster_properties = np.array(true_cluster_properties, dtype=np.float)\n",
    "    y_properties = torch.from_numpy(true_cluster_properties).to(clusters.device).float()\n",
    "    print(y_properties)    \n",
    "    \n",
    "    #print('match_cluster_targets')\n",
    "    return pred_indices, y_properties\n",
    "\n",
    "class SimpleEmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=16, ncats_out=2, nprops_out=1, output_dim=8,\n",
    "                 conv_depth=3, edgecat_depth=6, property_depth=3, k=8, aggr='add',\n",
    "                 norm=torch.tensor([1./500., 1./500., 1./54., 1/25., 1./1000.])):\n",
    "        super(SimpleEmbeddingNetwork, self).__init__()\n",
    "        \n",
    "#         self.datanorm = nn.Parameter(norm, requires_grad=False)\n",
    "        self.k = k\n",
    "        \n",
    "        start_width = 2 * (hidden_dim )\n",
    "        middle_width = (3 * hidden_dim ) // 2\n",
    "          \n",
    "        # embedding loss\n",
    "        self.inputnet =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            #nn.LayerNorm(hidden_dim),\n",
    "            nn.BatchNorm1d(num_features=hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            #nn.LayerNorm(hidden_dim),\n",
    "            nn.BatchNorm1d(num_features=hidden_dim)\n",
    "        )        \n",
    "        \n",
    "        self.edgeconvs = nn.ModuleList()\n",
    "        for i in range(conv_depth):\n",
    "            convnn = nn.Sequential(\n",
    "                nn.Linear(start_width, middle_width),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(middle_width, middle_width),                                             \n",
    "                nn.ELU(),\n",
    "                nn.Linear(middle_width, hidden_dim),                                             \n",
    "                nn.ELU(),\n",
    "                #nn.LayerNorm(hidden_dim),\n",
    "                nn.BatchNorm1d(num_features=hidden_dim)\n",
    "            )\n",
    "            self.edgeconvs.append(EdgeConv(nn=convnn, aggr=aggr))\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            #nn.LayerNorm(hidden_dim),\n",
    "            nn.BatchNorm1d(num_features=hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        # edge categorization\n",
    "        self.inputnet_cat =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),            \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(num_features=hidden_dim),\n",
    "            nn.Tanh(),            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(num_features=hidden_dim),\n",
    "            nn.Tanh()            \n",
    "        )\n",
    "        \n",
    "        self.edgecatconvs = nn.ModuleList()\n",
    "        for i in range(edgecat_depth):\n",
    "            convnn = nn.Sequential(\n",
    "                nn.Linear(start_width + 2*hidden_dim + 2*input_dim, middle_width),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(middle_width, hidden_dim),                                             \n",
    "                nn.ELU(),\n",
    "                nn.BatchNorm1d(num_features=hidden_dim)\n",
    "            )\n",
    "            self.edgecatconvs.append(EdgeConv(nn=convnn, aggr=aggr))\n",
    "        \n",
    "        self.edge_classifier = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm1d(num_features=hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm1d(num_features=hidden_dim//2),\n",
    "            nn.Linear(hidden_dim//2, ncats_out)\n",
    "        )\n",
    "        \n",
    "        # property prediction\n",
    "        self.inputnet_prop =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),            \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(num_features=hidden_dim),\n",
    "            nn.Tanh(),            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(num_features=hidden_dim),\n",
    "            nn.Tanh()            \n",
    "        )\n",
    "        \n",
    "        self.propertyconvs = nn.ModuleList()\n",
    "        for i in range(property_depth):\n",
    "            convnn = nn.Sequential(\n",
    "                nn.Linear(start_width + 2*hidden_dim + 2*input_dim, middle_width),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(middle_width, hidden_dim),                                             \n",
    "                nn.ELU(),\n",
    "                nn.BatchNorm1d(num_features=hidden_dim)\n",
    "            )\n",
    "            self.propertyconvs.append(EdgeConv(nn=convnn, aggr='max'))\n",
    "\n",
    "        self.property_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim//2, nprops_out)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x, batch: OptTensor=None):\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.size()[0], dtype=torch.int64, device=x.device)\n",
    "        \n",
    "        x_emb = self.inputnet(x)        \n",
    "        \n",
    "        for ec in self.edgeconvs:\n",
    "            edge_index = knn_graph(x_emb, self.k, batch, loop=False, flow=ec.flow)\n",
    "            x_emb = x_emb + ec(x_emb, edge_index)\n",
    "        \n",
    "        out = self.output(x_emb)\n",
    "        edge_index = knn_graph(out, self.k, batch, loop=False, flow=ec.flow)\n",
    "        \n",
    "        # use the embedded space to build an edge classifier\n",
    "        x_cat = self.inputnet_cat(x) + x_emb\n",
    "        for ec in self.edgecatconvs:            \n",
    "            x_cat = x_cat + ec(torch.cat([x_cat, x_emb, x], dim=1), edge_index)\n",
    "        \n",
    "        edge_scores = self.edge_classifier(torch.cat([x_cat[edge_index[0]], \n",
    "                                                      x_cat[edge_index[1]]], \n",
    "                                                      dim=1)).squeeze()\n",
    "        \n",
    "        # use the predicted graph to generate disjoint subgraphs\n",
    "        # these are our physics objects\n",
    "        objects =UnionFind(x.size()[0])\n",
    "        good_edges = edge_index[:,torch.argmax(edge_scores, dim=1) > 0]\n",
    "        good_edges_cpu = good_edges.cpu().numpy()        \n",
    "        for edge in good_edges_cpu.T:\n",
    "            objects.union(edge[0],edge[1])\n",
    "        cluster_map = torch.from_numpy(np.array([objects.find(i) for i in range(x.shape[0])], \n",
    "                                                dtype=np.int64)).to(x.device)\n",
    "        cluster_roots, inverse = torch.unique(cluster_map, return_inverse=True)\n",
    "        # remap roots to [0, ..., nclusters-1]\n",
    "        cluster_map = torch.arange(cluster_roots.size()[0], \n",
    "                                   dtype=torch.int64, \n",
    "                                   device=x.device)[inverse]\n",
    "        \n",
    "        x_prop = self.inputnet_prop(x) + x_emb\n",
    "        # now we accumulate over all selected disjoint subgraphs\n",
    "        # to define per-object properties\n",
    "        for ec in self.propertyconvs:\n",
    "            x_prop = x_prop + ec(torch.cat([x_prop, x_emb, x], dim=1), good_edges)        \n",
    "        props_pooled, cluster_batch = max_pool_x(cluster_map, x_prop, batch)\n",
    "        cluster_props = self.property_predictor(props_pooled)    \n",
    "        \n",
    "        return out, edge_scores, edge_index, cluster_map, cluster_props, cluster_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "norm = torch.tensor([1./70., 1./5., 1./400.])\n",
    "output_dim = 2\n",
    "\n",
    "model = SimpleEmbeddingNetwork(input_dim=3, \n",
    "                               hidden_dim=32, \n",
    "                               output_dim=output_dim,\n",
    "                               ncats_out=2,\n",
    "                               nprops_out=1,\n",
    "                               conv_depth=3, \n",
    "                               edgecat_depth=6, \n",
    "                               k=8, \n",
    "                               aggr='add',\n",
    "                               norm=norm).to('cuda')\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "                         {'params': list(model.inputnet.parameters()) + list(model.edgeconvs.parameters()) + list(model.output.parameters())},\n",
    "                         {'params': list(model.inputnet_cat.parameters()) + list(model.edgecatconvs.parameters()) + list(model.edge_classifier.parameters()), 'lr': 0.0},\n",
    "                         {'params': list(model.inputnet_prop.parameters()) + list(model.propertyconvs.parameters()) + list(model.property_predictor.parameters()), 'lr': 0.0}\n",
    "                        ], lr=2.5e-3, weight_decay=1e-3)\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.65, patience=30)\n",
    "# sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, 20, eta_min=0)\n",
    "\n",
    "truth_cache = []\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "n_samples = 1\n",
    "\n",
    "n = 10*n_samples\n",
    "color_cycle = plt.cm.coolwarm(np.linspace(0.1,0.9,n))\n",
    "marker_hits = ['^','v','s','h']\n",
    "marker_centers = ['+','1','x','3']\n",
    "all_loss = []\n",
    "\n",
    "converged_embedding = False\n",
    "converged_categorizer = False\n",
    "\n",
    "make_plots = True\n",
    "\n",
    "for e in range(1000):\n",
    "    \n",
    "    if((e+1)%200==0):\n",
    "        print('epoch',e)\n",
    "        \n",
    "    avg_loss = 0\n",
    "    \n",
    "    if make_plots:\n",
    "        plt.clf()\n",
    "\n",
    "    if opt.param_groups[0]['lr'] < 1e-4 and not converged_embedding:\n",
    "        converged_embedding = True\n",
    "        opt.param_groups[1]['lr'] = 1e-4\n",
    "        opt.param_groups[1]['lr'] = 1e-3\n",
    "        \n",
    "    if opt.param_groups[1]['lr'] < 1e-4 and not converged_categorizer and converged_embedding:\n",
    "        converged_categorizer = True\n",
    "        opt.param_groups[2]['lr'] = 1e-3\n",
    "    \n",
    "    \n",
    "    print('data len', len(data))\n",
    "    print(data, data)\n",
    "    \n",
    "    for idata, d in enumerate(data[0:n_samples]):\n",
    "        d_gpu = d.to('cuda')\n",
    "        \n",
    "        y_orig = d_gpu.y\n",
    "        \n",
    "        d_gpu.x = d_gpu.x[d_gpu.y < 3] # just take the first three tracks\n",
    "        d_gpu.x = (d_gpu.x - torch.min(d_gpu.x, axis=0).values)/(torch.max(d_gpu.x, axis=0).values - torch.min(d_gpu.x, axis=0).values) # Normalise\n",
    "        d_gpu.y_particle_barcodes = d_gpu.y_particle_barcodes[d_gpu.y < 3]\n",
    "        d_gpu.y = d_gpu.y[d_gpu.y < 3]\n",
    "        \n",
    "        \n",
    "        coords, edge_scores, edges, cluster_map, cluster_props, cluster_batch = model(d_gpu.x)\n",
    "        '''\n",
    "        #------------  DANIEL TRAINING VERSION ------------------\n",
    "        reference = coords.index_select(0, e_spatial[1])\n",
    "        neighbors = coords.index_select(0, e_spatial[0])\n",
    "        d = torch.sum((reference - neighbors)**2, dim=-1)\n",
    "        \n",
    "        hinge_truth = (d_gpu.y[e_spatial[0]] == d_gpu.y[e_spatial[1]]).float()\n",
    "        hinge_truth[hinge_truth == 0] = -1\n",
    "        print(hinge_truth)\n",
    "        \n",
    "        loss = torch.nn.functional.hinge_embedding_loss(d, hinge_truth, margin=1.0, reduction=\"mean\")\n",
    "        #==============================================\n",
    "        '''\n",
    "        \n",
    "        #-------------- LINDSEY TRAINING VERSION ------------------\n",
    "        print('compute hinge loss ...')\n",
    "        multi_simple_hinge = simple_embedding_truth(coords, d_gpu.y, device='cuda')\n",
    "        \n",
    "        print('scatter mean ...')\n",
    "        centers = scatter_mean(coords, d_gpu.y, dim=0, dim_size=(torch.max(d_gpu.y).item()+1))\n",
    "        \n",
    "        if make_plots:\n",
    "            if output_dim==3:\n",
    "                fig = plt.figure(figsize=(20,20))\n",
    "                ax = fig.add_subplot(111, projection='3d')\n",
    "                for i in range(centers.size()[0]):  \n",
    "                    ax.scatter(coords[d_gpu.y == i,0].cpu().detach().numpy(), \n",
    "                        coords[d_gpu.y == i,1].cpu().detach().numpy(),\n",
    "                        coords[d_gpu.y == i,2].cpu().detach().numpy(),\n",
    "                        color=color_cycle[2*idata + i], marker = marker_hits[i%4], s=100);\n",
    "\n",
    "                    ax.scatter(centers[i,0].cpu().detach().numpy(), \n",
    "                        centers[i,1].cpu().detach().numpy(), \n",
    "                        centers[i,2].cpu().detach().numpy(), \n",
    "                        marker=marker_centers[i%4], color=color_cycle[2*idata+i], s=100); \n",
    "            elif output_dim==2:\n",
    "                for i in range(centers.size()[0]):\n",
    "                        plt.scatter(coords[d_gpu.y == i,0].cpu().detach().numpy(), \n",
    "                                    coords[d_gpu.y == i,1].cpu().detach().numpy(),\n",
    "                                    color=color_cycle[2*idata + i], marker = marker_hits[i%4]);\n",
    "                        plt.scatter(centers[i,0].cpu().detach().numpy(), \n",
    "                                    centers[i,1].cpu().detach().numpy(), \n",
    "                                    marker=marker_centers[i%4], color=color_cycle[2*idata+i])  ;  \n",
    "    \n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())        \n",
    "        \n",
    "        hinges = torch.cat([F.hinge_embedding_loss(d**2, y, margin=1.0, reduction='mean')[None] \n",
    "                            for d, y in multi_simple_hinge],\n",
    "                           dim=0)\n",
    "        \n",
    "        y_edgecat = (d_gpu.y[edges[0]] == d_gpu.y[edges[1]]).long()\n",
    "        loss_ce = F.cross_entropy(edge_scores, y_edgecat, reduction='mean')\n",
    "        \n",
    "        pred_cluster_match, y_properties = match_cluster_targets(cluster_map, d_gpu.y, d_gpu)\n",
    "        \n",
    "        loss_mse = F.mse_loss(cluster_props[pred_cluster_match].squeeze(), y_properties, reduction='mean')\n",
    "        \n",
    "        loss = hinges.mean() + loss_ce + loss_mse\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        all_loss.append(avg_loss)\n",
    "        \n",
    "        loss.backward()\n",
    "                \n",
    "        print(e, idata, 'loss / LR /centers -->>\\n', \n",
    "              loss.item(), loss_ce.item(), loss_mse.item(), \n",
    "              '\\n', opt.param_groups[0]['lr'], opt.param_groups[1]['lr'], opt.param_groups[2]['lr'],\n",
    "              '\\n', 'N_true_edges / accuracy -->> ', y_edgecat.sum().item(), '/', (torch.argmax(edge_scores, dim=1) == y_edgecat).sum().item()/(y_edgecat.size()[0]),\n",
    "              '\\n centers --> ', centers,\n",
    "              '\\n cluster_properties -> ', cluster_props\n",
    "             )\n",
    "        #time.sleep(3.)\n",
    "    opt.step()\n",
    "    sched.step(avg_loss)\n",
    "\n",
    "print('complete', y_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1/y_properties)\n",
    "print(pred_cluster_match)\n",
    "print(1/cluster_props[pred_cluster_match].squeeze())\n",
    "\n",
    "n_clusters = data[0].y[data[0].y < 3].max().item() + 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    mapped_i = pred_cluster_match[i].item()\n",
    "    r = data[0].x[data[0].y < 3][cluster_map == mapped_i,0].cpu().detach().numpy()\n",
    "    phi = data[0].x[data[0].y < 3][cluster_map == mapped_i,1].cpu().detach().numpy()\n",
    "    z = data[0].x[data[0].y < 3][cluster_map == mapped_i,2].cpu().detach().numpy()\n",
    "    ax.scatter(r*np.cos(phi), \n",
    "               r*np.sin(phi),\n",
    "               color=color_cycle[2*idata + i], marker = marker_hits[i%4], s=100);\n",
    "    ax.text((r*np.cos(phi)).mean(), (r*np.sin(phi)).mean(), 'pt_pred = %.3f\\npt_true = %.3f' % (1./cluster_props[mapped_i].item(), 1/y_properties[i]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = UnionFind(coords.size()[0])\n",
    "good_edges = edges.t()[torch.argmax(edge_scores, dim=1) > 0].cpu().numpy()\n",
    "for edge in good_edges:\n",
    "    objects.union(edge[0],edge[1])\n",
    "    #objects.union(edge[1],edge[0])\n",
    "roots = np.array([objects.find(i) for i in range(coords.size()[0])], dtype=np.int64)\n",
    "print(roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(batch_loss, linewidth=5)\n",
    "plt.plot(batchlayer_loss)\n",
    "plt.plot(batchnosched_loss)\n",
    "plt.plot(batcheuclidean_loss)\n",
    "plt.plot(only_radius_graph_loss, linewidth=5)\n",
    "plt.plot(batchlayerallexamples_loss, linewidth=5)\n",
    "plt.plot(batchlayergraph_loss, linewidth=5)\n",
    "plt.plot(batchlayergraphdepth3_loss, linewidth=5)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend([\"Batch norm\", \"Batch and layer norm\", \"Batch norm and no scheduler\", \"Batch norm with Euclidean dist (vs. dist^2)\", \"No norms, No sched, Euclidean dist\", \"Batch and layer norm (on all pairs)\", \"Batch and layer norm with message passing (depth 1)\", \"Batch and layer norm with message passing (depth 3)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(batch_loss, linewidth=5)\n",
    "plt.plot(batchlayer_loss)\n",
    "plt.plot(batchnosched_loss)\n",
    "plt.plot(batcheuclidean_loss)\n",
    "plt.plot(only_radius_graph_loss, linewidth=5)\n",
    "plt.plot(batchlayerallexamples_loss, linewidth=5)\n",
    "plt.plot(batchlayergraph_loss, linewidth=5)\n",
    "plt.plot(batchlayergraphdepth3_loss, linewidth=5)\n",
    "\n",
    "plt.legend([\"Batch norm\", \"Batch and layer norm\", \"Batch norm and no scheduler\", \"Batch norm with Euclidean dist (vs. dist^2)\", \"No norms, No sched, Euclidean dist\", \"Batch and layer norm (on all pairs)\", \"Batch and layer norm with message passing (depth 1)\", \"Batch and layer norm with message passing (depth 3)\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
